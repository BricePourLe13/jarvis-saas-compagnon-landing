"use client"

import { useState, useCallback, useRef, useEffect } from 'react'
import { executeJarvisFunction } from '@/lib/jarvis-expert-functions'
import { getRealtimeURL } from '@/lib/openai-config'

interface VoiceVitrineConfig {
  onStatusChange?: (status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => void
  onTranscriptUpdate?: (transcript: string) => void
  maxDuration?: number // en secondes
}

export function useVoiceVitrineChat({
  onStatusChange,
  onTranscriptUpdate,
  maxDuration = 300 // 5 minutes par d√©faut
}: VoiceVitrineConfig) {
  // √âtats
  const [isConnected, setIsConnected] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  
  // Refs pour WebRTC
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const dataChannelRef = useRef<RTCDataChannel | null>(null)
  const audioElementRef = useRef<HTMLAudioElement | null>(null)
  const sessionStartTimeRef = useRef<number | null>(null)
  const maxDurationRef = useRef(maxDuration)

  // Mettre √† jour maxDuration
  useEffect(() => {
    maxDurationRef.current = maxDuration
  }, [maxDuration])

  const updateStatus = useCallback((status: 'idle' | 'connecting' | 'connected' | 'listening' | 'speaking' | 'error') => {
    onStatusChange?.(status)
  }, [onStatusChange])

  const updateTranscript = useCallback((transcript: string) => {
    setCurrentTranscript(transcript)
    onTranscriptUpdate?.(transcript)
  }, [onTranscriptUpdate])

  // üéØ Handler pour les function calls (ROI, success stories, etc.)
  const handleFunctionCall = useCallback(async (message: any, dataChannel: RTCDataChannel) => {
    try {
      const { call_id, name, arguments: argsString } = message
      console.log(`üîß Ex√©cution function: ${name}`)
      console.log(`üìä Arguments:`, argsString)
      
      // Parser les arguments
      const args = JSON.parse(argsString)
      
      // Ex√©cuter la fonction experte
      const result = await executeJarvisFunction(name, args)
      console.log(`‚úÖ R√©sultat function ${name}:`, result)
      
      // Renvoyer le r√©sultat √† l'IA
      dataChannel.send(JSON.stringify({
        type: 'conversation.item.create',
        item: {
          type: 'function_call_output',
          call_id: call_id,
          output: JSON.stringify(result)
        }
      }))
      
      // Demander √† l'IA de r√©pondre avec ce r√©sultat
      dataChannel.send(JSON.stringify({
        type: 'response.create'
      }))
      
      console.log('üì§ R√©sultat envoy√© √† JARVIS pour formulation')
      
    } catch (error) {
      console.error('‚ùå Erreur ex√©cution function call:', error)
      
      // En cas d'erreur, informer l'IA
      if (message.call_id) {
        dataChannel.send(JSON.stringify({
          type: 'conversation.item.create',
          item: {
            type: 'function_call_output',
            call_id: message.call_id,
            output: JSON.stringify({ 
              error: 'Erreur lors du calcul, je vais vous donner une estimation g√©n√©rale.' 
            })
          }
        }))
        
        dataChannel.send(JSON.stringify({
          type: 'response.create'
        }))
      }
    }
  }, [])

  // Cr√©er une session √©ph√©m√®re pour la d√©mo
  const createDemoSession = useCallback(async () => {
    // L'API attend maintenant directement la config sans wrapper "session"
    const response = await fetch('/api/voice/vitrine/session', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({}), // L'API cr√©e la config elle-m√™me maintenant
    })

    if (!response.ok) {
      // üîí NOUVEAU : G√©rer les erreurs de limitation
      const errorData = await response.json().catch(() => ({}))
      
      const error: any = new Error(errorData.error || `Erreur session: ${response.status}`)
      error.statusCode = response.status
      error.hasActiveSession = errorData.hasActiveSession
      error.remainingCredits = errorData.remainingCredits
      error.isBlocked = errorData.isBlocked
      
      throw error
    }

    const sessionData = await response.json()
    console.log('‚úÖ Session cr√©√©e:', sessionData)
    console.log('üîç Structure session:', JSON.stringify(sessionData.session, null, 2))
    
    // ‚úÖ FORMAT GA : client_secret est maintenant un objet { value, expires_at }
    // Doc ligne 360-361: console.log(data.value)
    const ephemeralKey = typeof sessionData.session?.client_secret === 'object' 
      ? sessionData.session.client_secret.value 
      : sessionData.session?.client_secret
      
    console.log('üîç ephemeral key:', ephemeralKey?.substring(0, 15) + '...')
    
    // üí≥ Retourner aussi les cr√©dits restants
    if (sessionData.remainingCredits !== undefined) {
      console.log(`üí≥ Cr√©dits restants: ${sessionData.remainingCredits} minutes`)
    }
    
    return {
      ...sessionData,
      // Exposer directement le ephemeral key pour compatibilit√©
      ephemeralKey
    }
  }, [])

  // Initialiser WebRTC
  const initializeWebRTC = useCallback(async () => {
    try {
      // V√©rifier support WebRTC
      if (!window.RTCPeerConnection) {
        throw new Error('WebRTC non support√© par ce navigateur')
      }

      // Cr√©er session d√©mo
      const sessionResponse = await createDemoSession()
      const session = sessionResponse.session
      
      if (!sessionResponse?.ephemeralKey) {
        throw new Error('Session cr√©√©e mais token manquant')
      }
      
      console.log('üîç Session utilis√©e:', session)
      console.log('üîç Ephemeral key:', sessionResponse.ephemeralKey?.substring(0, 15) + '...')
      
      // Configurer peer connection
      const pc = new RTCPeerConnection({
        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
      })
      peerConnectionRef.current = pc

      // Cr√©er l'√©l√©ment audio pour le playback (COMME LE KIOSK)
      if (!audioElementRef.current) {
        const audioEl = document.createElement('audio')
        audioEl.id = 'jarvis-audio-vitrine'
        audioEl.autoplay = true
        audioEl.controls = false // true pour debug
        audioEl.muted = false  // üîß FIX: Explicitement NON mut√©
        audioEl.volume = 1.0   // üîß FIX: Volume √† 100%
        audioEl.setAttribute('playsinline', '')  // üîß FIX: iOS compatibility
        
        // üîß FIX CRITIQUE : Ajouter au DOM pour que autoplay fonctionne
        document.body.appendChild(audioEl)
        
        // üîß FIX : Ajouter listeners debug
        audioEl.onloadedmetadata = () => {
          console.log('‚úÖ [AUDIO] Metadata charg√© - pr√™t √† jouer')
        }
        audioEl.onerror = (e) => {
          console.error('‚ùå [AUDIO] Erreur audio element:', e)
        }
        audioEl.onplay = () => {
          console.log('‚ñ∂Ô∏è [AUDIO] Playback d√©marr√©')
        }
        audioEl.onended = () => {
          console.log('üèÅ [AUDIO] Audio termin√©')
        }
        
        audioElementRef.current = audioEl
      }

      // G√©rer l'audio entrant (r√©ponses de JARVIS) - DIAGNOSTIC COMPLET
      pc.ontrack = (event) => {
        console.log(`üéµ TRACK EVENT FIRED: ${event.track.kind} (streams: ${event.streams.length})`)
        
        // ‚úÖ CRITICAL: V√©rifier que c'est bien un track AUDIO
        if (event.track.kind !== 'audio') {
          console.warn(`‚ö†Ô∏è Track ignor√© (type: ${event.track.kind})`)
          return
        }
        
        if (!audioElementRef.current) {
          console.error('‚ùå Audio element n\'existe pas!')
          return
        }
        
        if (!event.streams[0]) {
          console.error('‚ùå Aucun stream dans l\'event!')
          return
        }
        
        // Logger l'√©tat AVANT assignation
        console.log(`üìä Audio element AVANT - srcObject active: ${audioElementRef.current.srcObject?.active || false}, paused: ${audioElementRef.current.paused}, muted: ${audioElementRef.current.muted}`)
        
        // ‚úÖ Assigner le stream
        audioElementRef.current.srcObject = event.streams[0]
        
        // Logger l'√©tat APR√àS assignation
        console.log(`üìä Audio element APR√àS - srcObject active: ${audioElementRef.current.srcObject?.active || false}`)
        
        // üîß FIX CRITIQUE: Resume AudioContext pour d√©bloquer autoplay
        try {
          const audioCtx = new (window.AudioContext || (window as any).webkitAudioContext)()
          if (audioCtx.state === 'suspended') {
            audioCtx.resume().then(() => {
              console.log('‚úÖ [AUDIO] AudioContext resumed (autoplay d√©bloqu√©)')
            })
          }
        } catch (err) {
          console.warn(`‚ö†Ô∏è [AUDIO] AudioContext non disponible: ${err}`)
        }
        
        // ‚úÖ FORCER play() imm√©diatement
        const playPromise = audioElementRef.current.play()
        
        if (playPromise !== undefined) {
          playPromise
            .then(() => {
              console.log('‚úÖ ‚ñ∂Ô∏è Audio playback D√âMARR√â avec succ√®s!')
            })
            .catch((err) => {
              console.error(`‚ùå PLAY FAILED: ${err.name} - ${err.message}`)
              
              if (err.name === 'NotAllowedError') {
                console.warn('‚ö†Ô∏è Autoplay bloqu√© par le navigateur - Cliquez pour activer l\'audio')
                // Fallback: attendre interaction utilisateur
                document.addEventListener('click', () => {
                  console.log('üñ±Ô∏è Click d√©tect√© - Tentative play()...')
                  audioElementRef.current?.play()
                    .then(() => console.log('‚úÖ Audio d√©marr√© apr√®s click'))
                    .catch(e => console.error(`‚ùå √âchec apr√®s click: ${e.message}`))
                }, { once: true })
              }
            })
        }
      }

      // Demander acc√®s microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 16000, // ‚úÖ Standard OpenAI Realtime (16 kHz PCM16 mono)
            channelCount: 1,
            latency: 0.01, // Faible latence
            volume: 1.0
          }
        })
        
        // Ajouter track audio
        const audioTrack = stream.getAudioTracks()[0]
        pc.addTrack(audioTrack, stream)
      } catch (micError) {
        throw new Error('MICROPHONE_PERMISSION_DENIED')
      }

      // Configurer data channel
      const dc = pc.createDataChannel('oai-events')
      dataChannelRef.current = dc

      dc.onopen = () => {
        console.log('‚úÖ Data channel ouvert')
        setIsConnected(true)
        updateStatus('connected')
        sessionStartTimeRef.current = Date.now()
        
        // ‚úÖ GA : Envoyer session.update avec config compl√®te
        if (sessionResponse.sessionUpdateConfig) {
          console.log('üì§ Envoi session.update (GA)')
          dc.send(JSON.stringify({
            type: 'session.update',
            session: sessionResponse.sessionUpdateConfig
          }))
          console.log('‚úÖ [VITRINE GA] Session configur√©e (config envoy√©e via data channel)')
        } else {
          console.warn('‚ö†Ô∏è sessionUpdateConfig manquant - mode Beta')
          console.log('‚úÖ [VITRINE BETA] Session pr√™te (config envoy√©e c√¥t√© serveur)')
        }
      }

      dc.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data)
          console.log('üì® Message re√ßu:', message.type)
          
          switch (message.type) {
            case 'input_audio_buffer.speech_started':
              updateStatus('listening')
              break
              
            case 'input_audio_buffer.speech_stopped':
              updateStatus('connected')
              break
              
            case 'response.created':
              updateStatus('speaking')
              setIsAISpeaking(true)
              break
              
            case 'response.done':
              updateStatus('connected')
              setIsAISpeaking(false)
              break
              
            case 'response.output_audio.delta':
              console.log('üé§ Chunk audio re√ßu de JARVIS (GA)')
              // Audio chunks from GA API - traitement imm√©diat
              break
              
            case 'conversation.item.input_audio_transcription.completed':
              if (message.transcript) {
                updateTranscript(message.transcript)
              }
              break
              
            case 'response.output_text.delta':
              // Text chunks from GA API
              break
            
            // üéØ NOUVEAU : Function calling pour JARVIS VITRINE (commercial expert)
            case 'response.function_call_arguments.delta':
              // Arguments de fonction re√ßus progressivement
              console.log('üîß Function call arguments delta:', message)
              break
            
            case 'response.function_call_arguments.done':
              // Function call complet - EX√âCUTER
              console.log('üéØ Function call complet:', message)
              handleFunctionCall(message, dc)
              break
            
            case 'response.output_item.done':
              // Item termin√© (peut contenir un function call)
              if (message.item?.type === 'function_call') {
                console.log('‚úÖ Function call item done:', message.item.name)
              }
              break
              
            case 'error':
              console.error('‚ùå Erreur OpenAI:', message)
              setError(message.error?.message || 'Erreur de conversation')
              updateStatus('error')
              break
          }
        } catch (parseError) {
          console.error('‚ùå Erreur parsing message:', parseError)
        }
      }

      dc.onclose = () => {
        console.log('üì° Data channel ferm√©')
        setIsConnected(false)
        updateStatus('idle')
      }

      dc.onerror = (error) => {
        console.error('‚ùå Erreur data channel:', error)
        setError('Erreur de communication')
        updateStatus('error')
      }

      // Cr√©er et envoyer offre
      const offer = await pc.createOffer()
      await pc.setLocalDescription(offer)

      // Envoyer √† OpenAI (format GA)
      const ephemeralKey = sessionResponse.ephemeralKey
      console.log('üîë Token utilis√©:', ephemeralKey?.substring(0, 20) + '...')
      // üéØ Vitrine utilise le mod√®le full pour meilleure qualit√© d√©mo
      // ‚úÖ FORMAT GA : Pas de header Beta pour gpt-realtime-2025-08-28
      const realtimeResponse = await fetch(getRealtimeURL('vitrine'), {
        method: 'POST',
        body: offer.sdp,
        headers: {
          'Authorization': `Bearer ${ephemeralKey}`,
          'Content-Type': 'application/sdp',
          // ‚ùå SUPPRIM√â: 'OpenAI-Beta': 'realtime=v1' (format GA)
        },
      })

      if (!realtimeResponse.ok) {
        throw new Error(`WebRTC setup failed: ${realtimeResponse.status}`)
      }

      const answerSdp = await realtimeResponse.text()
      await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp })

      console.log('‚úÖ WebRTC initialis√© avec succ√®s')
      
      // üí≥ Retourner les donn√©es de session
      return sessionResponse
      
    } catch (error: any) {
      console.error('‚ùå Erreur WebRTC:', error)
      
      // ‚úÖ PROPAGER LE MESSAGE D'ERREUR DE L'API (limite quotidienne, etc.)
      let errorMessage = error.message || 'Erreur de connexion'
      
      // Si l'erreur vient de createDemoSession, utiliser le message de l'API
      if (error.statusCode === 429 || error.statusCode === 403 || error.statusCode === 409) {
        // Erreur de limitation ou blocage - message d√©j√† format√© par l'API
        errorMessage = error.message
      } else {
        // Autres erreurs - formater le message
        switch (error.message) {
          case 'MICROPHONE_PERMISSION_DENIED':
            errorMessage = 'Veuillez autoriser l\'acc√®s au microphone'
            break
          case 'WebRTC non support√© par ce navigateur':
            errorMessage = 'Navigateur incompatible. Utilisez Chrome, Firefox ou Safari r√©cent'
            break
          default:
            if (error.name === 'NotAllowedError') {
              errorMessage = 'Acc√®s microphone refus√©'
            } else if (error.message && error.message !== 'Erreur de connexion') {
              // Garder le message original si pr√©sent
              errorMessage = error.message
            }
            break
        }
      }
      
      setError(errorMessage)
      updateStatus('error')
      
      // ‚úÖ Propager l'erreur avec le bon message
      const finalError: any = new Error(errorMessage)
      finalError.statusCode = error.statusCode
      finalError.hasActiveSession = error.hasActiveSession
      finalError.remainingCredits = error.remainingCredits
      finalError.isBlocked = error.isBlocked
      throw finalError
    }
  }, [createDemoSession, updateStatus, updateTranscript])

  // Connexion
  const connect = useCallback(async () => {
    if (isConnected) return
    
    setError(null)
    updateStatus('connecting')
    
    try {
      const sessionData = await initializeWebRTC()
      // üí≥ Retourner les donn√©es de session (incluant remainingCredits)
      return sessionData
    } catch (error: any) {
      console.error('Erreur de connexion:', error)
      
      // ‚úÖ PROPAGER LE MESSAGE D'ERREUR CORRECTEMENT
      const errorMessage = error.message || 'Erreur de connexion'
      setError(errorMessage)
      
      // R√©initialiser l'√©tat en cas d'erreur pour √©viter la boucle
      updateStatus('error')
      setIsConnected(false)
      
      // Propager l'erreur avec toutes les m√©tadonn√©es
      const finalError: any = new Error(errorMessage)
      finalError.statusCode = error.statusCode
      finalError.hasActiveSession = error.hasActiveSession
      finalError.remainingCredits = error.remainingCredits
      finalError.isBlocked = error.isBlocked
      finalError.resetTime = error.resetTime
      throw finalError
    }
  }, [isConnected, initializeWebRTC, updateStatus])

  // D√©connexion
  const disconnect = useCallback(async () => {
    try {
      // üîí NOUVEAU : Comptabiliser le temps de session
      if (sessionStartTimeRef.current) {
        const durationSeconds = Math.floor((Date.now() - sessionStartTimeRef.current) / 1000)
        console.log(`‚è±Ô∏è Dur√©e session: ${durationSeconds}s (${Math.ceil(durationSeconds / 60)} cr√©dits)`)
        
        // Appeler l'API pour enregistrer la dur√©e
        try {
          const response = await fetch('/api/voice/vitrine/end-session', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ durationSeconds })
          })
          
          if (response.ok) {
            console.log('‚úÖ Dur√©e session enregistr√©e')
          }
        } catch (error) {
          console.error('‚ùå Erreur enregistrement dur√©e:', error)
        }
      }
      
      // Fermer data channel
      if (dataChannelRef.current) {
        dataChannelRef.current.close()
        dataChannelRef.current = null
      }

      // Fermer peer connection
      if (peerConnectionRef.current) {
        peerConnectionRef.current.close()
        peerConnectionRef.current = null
      }

      // Arr√™ter audio et nettoyer DOM
      if (audioElementRef.current) {
        // Pause et reset
        audioElementRef.current.pause()
        audioElementRef.current.srcObject = null
        
        // Retirer du DOM
        if (audioElementRef.current.parentNode) {
          audioElementRef.current.parentNode.removeChild(audioElementRef.current)
          console.log('üßπ [AUDIO] Audio element retir√© du DOM')
        }
        audioElementRef.current = null
      }

      setIsConnected(false)
      setCurrentTranscript('')
      setIsAISpeaking(false)
      sessionStartTimeRef.current = null
      updateStatus('idle')
      
    } catch (error) {
      console.error('Erreur de d√©connexion:', error)
    }
  }, [updateStatus])

  // Nettoyage au d√©montage
  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [disconnect])

  // V√©rification timeout de session
  useEffect(() => {
    if (!isConnected || !sessionStartTimeRef.current) return

    const checkTimeout = () => {
      if (sessionStartTimeRef.current) {
        const elapsed = (Date.now() - sessionStartTimeRef.current) / 1000
        if (elapsed >= maxDurationRef.current) {
          disconnect()
        }
      }
    }

    const interval = setInterval(checkTimeout, 1000)
    return () => clearInterval(interval)
  }, [isConnected, disconnect])

  return {
    // √âtats
    isConnected,
    error,
    currentTranscript,
    isAISpeaking,
    
    // Actions
    connect,
    disconnect
  }
}
